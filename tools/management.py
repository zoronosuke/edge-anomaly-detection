"""
ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†ãƒ»ç›£è¦–ç”¨ã®ãƒ„ãƒ¼ãƒ«
"""

import argparse
import json
from pathlib import Path
import pandas as pd
from datetime import datetime, timedelta
try:
    import matplotlib.pyplot as plt
    MATPLOTLIB_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False


def analyze_events(csv_path: str):
    """ã‚¤ãƒ™ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã®åˆ†æ"""
    try:
        df = pd.read_csv(csv_path)
        df['timestamp'] = pd.to_datetime(df['timestamp'])
        
        print("="*60)
        print("ğŸ“Š ã‚¤ãƒ™ãƒ³ãƒˆåˆ†æãƒ¬ãƒãƒ¼ãƒˆ")
        print("="*60)
        
        # åŸºæœ¬çµ±è¨ˆ
        total_events = len(df)
        total_anomalies = len(df[df['anomaly_flag'] == True])
        anomaly_rate = (total_anomalies / total_events * 100) if total_events > 0 else 0
        
        print(f"ğŸ“ˆ ç·ã‚¤ãƒ™ãƒ³ãƒˆæ•°: {total_events}")
        print(f"ğŸš¨ ç•°å¸¸æ¤œå‡ºæ•°: {total_anomalies}")
        print(f"ğŸ“Š ç•°å¸¸ç‡: {anomaly_rate:.2f}%")
        print()
        
        # ãƒ‡ãƒã‚¤ã‚¹åˆ¥çµ±è¨ˆ
        print("ğŸ–¥ï¸ ãƒ‡ãƒã‚¤ã‚¹åˆ¥çµ±è¨ˆ:")
        device_stats = df.groupby('device_id').agg({
            'event_id': 'count',
            'anomaly_flag': 'sum',
            'person_count': 'mean'
        }).round(2)
        device_stats.columns = ['ç·ã‚¤ãƒ™ãƒ³ãƒˆ', 'ç•°å¸¸æ•°', 'å¹³å‡äººæ•°']
        print(device_stats)
        print()
        
        # æ™‚é–“åˆ¥åˆ†æ
        df['hour'] = df['timestamp'].dt.hour
        hourly_stats = df.groupby('hour').agg({
            'event_id': 'count',
            'anomaly_flag': 'sum'
        })
        
        print("â° æ™‚é–“åˆ¥çµ±è¨ˆ (ä¸Šä½5æ™‚é–“):")
        top_hours = hourly_stats.sort_values('anomaly_flag', ascending=False).head()
        print(top_hours)
        print()
        
        # æœ€è¿‘ã®æ´»å‹•
        recent_threshold = datetime.now() - timedelta(hours=24)
        recent_df = df[df['timestamp'] > recent_threshold]
        
        print(f"ğŸ• éå»24æ™‚é–“ã®æ´»å‹•:")
        print(f"   ã‚¤ãƒ™ãƒ³ãƒˆæ•°: {len(recent_df)}")
        print(f"   ç•°å¸¸æ•°: {len(recent_df[recent_df['anomaly_flag'] == True])}")
        
        if len(recent_df) > 0:
            latest_event = recent_df.sort_values('timestamp').iloc[-1]
            print(f"   æœ€æ–°ã‚¤ãƒ™ãƒ³ãƒˆ: {latest_event['timestamp']} "
                  f"({latest_event['device_id']})")
        
    except FileNotFoundError:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")
    except Exception as e:
        print(f"âŒ åˆ†æã‚¨ãƒ©ãƒ¼: {e}")


def analyze_logs(json_path: str):
    """ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°ã®åˆ†æ"""
    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            logs = json.load(f)
        
        if not logs:
            print("ğŸ“ ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        print("="*60)
        print("ğŸ“ ã‚·ã‚¹ãƒ†ãƒ ãƒ­ã‚°åˆ†æ")
        print("="*60)
        
        # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«åˆ¥çµ±è¨ˆ
        levels = {}
        for log in logs:
            level = log.get('level', 'UNKNOWN')
            levels[level] = levels.get(level, 0) + 1
        
        print("ğŸ“Š ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«åˆ¥çµ±è¨ˆ:")
        for level, count in sorted(levels.items()):
            print(f"   {level}: {count}")
        print()
        
        # ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã®è©³ç´°
        error_logs = [log for log in logs if log.get('level') == 'ERROR']
        if error_logs:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼ãƒ­ã‚° (æœ€æ–°5ä»¶):")
            for log in error_logs[-5:]:
                timestamp = log.get('timestamp', 'N/A')
                message = log.get('message', 'N/A')
                device_id = log.get('device_id', 'N/A')
                print(f"   {timestamp} [{device_id}] {message}")
        else:
            print("âœ… ã‚¨ãƒ©ãƒ¼ãƒ­ã‚°ã¯ã‚ã‚Šã¾ã›ã‚“")
        
    except FileNotFoundError:
        print(f"âŒ ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {json_path}")
    except Exception as e:
        print(f"âŒ ãƒ­ã‚°åˆ†æã‚¨ãƒ©ãƒ¼: {e}")


def cleanup_old_data(data_dir: str, days: int = 30):
    """å¤ã„ãƒ‡ãƒ¼ã‚¿ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    data_path = Path(data_dir)
    cutoff_date = datetime.now() - timedelta(days=days)
    
    print(f"ğŸ§¹ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ— ({days}æ—¥å‰ã‚ˆã‚Šå¤ã„ãƒ‡ãƒ¼ã‚¿ã‚’å‰Šé™¤)")
    
    # å¤ã„ç”»åƒãƒ•ã‚¡ã‚¤ãƒ«ã®å‰Šé™¤
    images_dir = data_path / "images"
    if images_dir.exists():
        deleted_count = 0
        for image_file in images_dir.glob("*.jpg"):
            if image_file.stat().st_mtime < cutoff_date.timestamp():
                image_file.unlink()
                deleted_count += 1
        
        print(f"   å‰Šé™¤ã—ãŸç”»åƒãƒ•ã‚¡ã‚¤ãƒ«: {deleted_count}")
    
    # CSV ã®ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ã¨æœ€é©åŒ–
    events_csv = data_path / "events.csv"
    if events_csv.exists():
        try:
            df = pd.read_csv(events_csv)
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            # å¤ã„ãƒ¬ã‚³ãƒ¼ãƒ‰ã‚’é™¤å¤–
            df_filtered = df[df['timestamp'] > cutoff_date]
            removed_count = len(df) - len(df_filtered)
            
            if removed_count > 0:
                # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
                backup_path = events_csv.with_suffix('.backup.csv')
                df.to_csv(backup_path, index=False)
                
                # ãƒ•ã‚£ãƒ«ã‚¿ãƒ¼æ¸ˆã¿ãƒ‡ãƒ¼ã‚¿ã§æ›´æ–°
                df_filtered.to_csv(events_csv, index=False)
                
                print(f"   å‰Šé™¤ã—ãŸã‚¤ãƒ™ãƒ³ãƒˆãƒ¬ã‚³ãƒ¼ãƒ‰: {removed_count}")
                print(f"   ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_path}")
            else:
                print("   å‰Šé™¤å¯¾è±¡ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒ¬ã‚³ãƒ¼ãƒ‰ã¯ã‚ã‚Šã¾ã›ã‚“")
                
        except Exception as e:
            print(f"   âŒ CSVã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="System Management Tools")
    parser.add_argument("command", choices=["analyze", "logs", "cleanup"],
                       help="å®Ÿè¡Œã™ã‚‹ã‚³ãƒãƒ³ãƒ‰")
    parser.add_argument("--data-dir", default="./data",
                       help="ãƒ‡ãƒ¼ã‚¿ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (default: ./data)")
    parser.add_argument("--days", type=int, default=30,
                       help="ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¯¾è±¡æ—¥æ•° (default: 30)")
    
    args = parser.parse_args()
    
    data_dir = Path(args.data_dir)
    
    if args.command == "analyze":
        events_csv = data_dir / "events.csv"
        analyze_events(str(events_csv))
    
    elif args.command == "logs":
        logs_json = data_dir / "system_logs.json"
        analyze_logs(str(logs_json))
    
    elif args.command == "cleanup":
        cleanup_old_data(str(data_dir), args.days)


if __name__ == "__main__":
    main()
